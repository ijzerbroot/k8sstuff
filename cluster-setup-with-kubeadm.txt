
Kubernetes cluster setup using kubeadm

https://kubernetes.io/docs/setup/independent/install-kubeadm/
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Install Ubuntu 16.04 without swap
Set private network-adapter (192.168.1.10 for master)
sudo apt-get install openssh-server git
sudo systemctl enable ssh
sudo systemctl disable ufw
sudo systemctl stop ufw

sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get -y update
sudo apt-get -y install docker-ce
sudo apt-get -y install git docker-compose curl

ALL MACHINES:
sudo sysctl -w vm.max_map_count=262144

sudo su -


apt-get update && apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl

ON MASTER ONLY:
kubeadm init --apiserver-advertise-address=192.168.1.10 --pod-network-cidr=10.244.0.0/16
sysctl net.bridge.bridge-nf-call-iptables=1

#You can now join any number of machines by running the following on each node
#as root:
#
#  kubeadm join --token 5ed8b1.db01bdd9f6a2a390 192.168.1.10:6443 --discovery-token-ca-cert-hash sha256:0c7561c0c15ccb040eaaa86194e5feaf59ae0c08e81d048954eea832366cf3bd


If you are using VirtualBox (directly or via Vagrant), you will need to ensure that hostname -i returns a routable IP address (i.e. one on the second network interface, not the first one).

install a pod network addon:
export kubever=$(kubectl version | base64 | tr -d '\n')
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml

following command should show kube-dns pod running:
kubectl get pods --all-namespaces

For allowing pods to be schedukled on master itself:
kubectl taint nodes --all node-role.kubernetes.io/master-

join nodes with the command output by kubeadm

deploy kubernetes dashboard:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
OR without SSL:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes-dashboard.yaml

installeer eventueel heapster et al:
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml
#kubectl apply --namespace kube-system -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"
run heapster rbac:
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml

maar beter TICK stack en ELK



apply the dashboard rbac
proxy the dashboard:
kubectl proxy

make a tunnel and access it as localhost:
http://localhost:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/

setup shared storage. Example Gluster (from https://launchpad.net/~gluster):

on gluster server:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-server

on nodes:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-client

# glusterfs uses /var/lib/glusterd for state
# Make gluster storage volume:

parted -s /dev/xvdz mklabel msdos
parted -s /dev/xvdz mkpart primary 1 100%

mkfs.xfs -i size=512 /dev/xvdz1
mkdir -p /data/brick1
echo '/dev/xvdz1 /data/brick1 xfs defaults 1 2' >> /etc/fstab
mount -a && mount

# let firewall accept all traffic from other nodes:
# iptables -I INPUT -p all -s <peer ip-address> -j ACCEPT
# configure trusted pool on each server:  gluster peer probe <peer>

# on each server make subdir:
mkdir -p /data/brick1/gv0

# on any one of the servers create volume:
gluster volume create gv0 replica 2 server1:/data/brick1/gv0 server2:/data/brick1/gv0
gluster volume start gv0
gluster volume info
# test it
# mount -t glusterfs server1:/gv0 /mnt

chown 2000 /data/brick1/gv0

add the glusterfs endpoints (server addresses)
kubectl create -f glusterfs-endpoints.json
verify:
kubectl get endpoints

# create a service for the glusterfs endpoints so that they will persist:
kubectl create -f glusterfs-service.json


# To use glusterfs storage in a pod (path is glusterfs volume name):
#"volumes": [
#  {
#    "name": "glusterfsvol",
#    "glusterfs": {
#      "endpoints": "glusterfs-cluster",
#      "path": "kube_vol",
#      "readWrite": true
#    }
#  }
#]

# to see if gluster volumes are used:
#kubectl exec glusterfs -- mount | grep gluster


# OR RUN gluster in pods! https://github.com/gluster/gluster-kubernetes

install traefik
kubectl apply -f traefik-role.yaml
kubectl apply -f traefik-daemonset.yaml
kubectl apply -f traefik-web-ui.yaml

build custom Kibana-image to support Logtrail from Kibana dockerfile on every node as "localhost/kibana:1"

monitoring:
kubectl apply -f es-clusterrole.yml
kubectl apply -f es-serviceaccount.yml
kubectl apply -f es-clusterrolebinding.yml
#kubectl apply -f es-statefulset.yml
#kubectl apply -f es-pv.yml
#kubectl apply -f es-pvclaim.yml
kubectl create -f es-deployment.yml
kubectl create -f es-service.yml
kubectl create -f es-ingress.yaml
kubectl create -f logstash-config.yaml
kubectl create -f logstashyaml-config.yaml
kubectl create -f logstash-deployment.yaml
kubectl create -f logstash-service.yaml
kubectl create -f filebeat-config.yaml
kubectl create -f filebeat-ds.yaml
kubectl create -f kibana-deployment.yaml
kubectl create -f kibana-service.yaml
kubectl create -f kibana-ingress.yaml
#kubectl apply -f https://raw.githubusercontent.com/elastic/beats/master/deploy/kubernetes/filebeat-kubernetes.yaml
# check status
kubectl --namespace=kube-system get ds/filebeat
