
Kubernetes cluster setup using kubeadm

https://kubernetes.io/docs/setup/independent/install-kubeadm/
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Install Ubuntu 16.04 without swap
Set private network-adapter (192.168.1.10 for master)
sudo apt-get install openssh-server git
sudo systemctl enable ssh
sudo systemctl disable ufw
sudo systemctl stop ufw

sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get -y update
sudo apt-get -y install docker-ce
sudo apt-get -y install git docker-compose curl

# Voor Centos:
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum -y install docker-ce epel-release
sudo systemctl enable docker && sudo systemctl start docker
sudo yum install -y python-pip curl
sudo pip install docker-compose
disable selinux
sudo systemctl disable firewalld

ALL MACHINES:
sudo sysctl -w vm.max_map_count=262144
sudo sysctl -w net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward = 1

sudo su -

apt-get update && apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl

# Voor Centos:
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
setenforce 0
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
reboot

ON MASTER ONLY:
kubeadm init --apiserver-advertise-address=192.168.1.10 --pod-network-cidr=10.244.0.0/16


install a pod network addon:
export kubever=$(kubectl version | base64 | tr -d '\n')
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

#kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/rbac.yaml
#kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/canal.yaml


following command should show kube-dns pod running:
kubectl get pods --all-namespaces

wait until stabilized
#You can now join any number of machines by running the following on each node
#as root:
#
#  kubeadm join --token 003f03.441f4f6c5d87d22c 192.168.1.10:6443 --discovery-token-ca-cert-hash sha256:5b7159073875aa9f9f947173c169dfc10c93b3813c56ee2d60c308747e23fd2f


If you are using VirtualBox (directly or via Vagrant), you will need to ensure that hostname -i returns a routable IP address. also the first ethernet adapter should be a cluster-reachable one (internal network)

For allowing pods to be scheduled on master itself:
kubectl taint nodes --all node-role.kubernetes.io/master-

join nodes with the command output by kubeadm

deploy kubernetes dashboard:
kubectl create -f dashboard-rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
OR without SSL:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes-dashboard.yaml

installeer eventueel heapster et al:
#kubectl create -f https://git.io/weave-kube
#$ kubectl create -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/monitoring-standalone/v1.2.0.yaml

#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml

install traefik
kubectl apply -f traefik-rbac.yaml
kubectl apply -f traefik-daemonset.yaml
kubectl apply -f traefik-web-ui.yaml

# weave scope
kubectl apply --namespace kube-system -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"
kubectl create -f weavescope-ingress.yaml
run heapster rbac:
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml

maar beter TICK stack en ELK

proxy the dashboard:
kubectl proxy

make a tunnel and access it as localhost:
http://localhost:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/

setup shared storage. Example Gluster (from https://launchpad.net/~gluster):

on gluster server:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-server
CENTOS:
yum -y install centos-release-gluster
yum -y install glusterfs-server
systemctl enable glusterd.service
systemctl start glusterd.service

on nodes:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-client
CENTOS:
yum -y install centos-release-gluster
yum -y install glusterfs
yum -y install glusterfs-fuse

# glusterfs uses /var/lib/glusterd for state
# Make gluster storage volume:

parted -s /dev/xvdz mklabel msdos
parted -s /dev/xvdz mkpart primary 1 100%

mkfs.xfs -i size=512 /dev/xvdz1
mkdir -p /data/brick1
echo '/dev/xvdz1 /data/brick1 xfs defaults 1 2' >> /etc/fstab
mount -a && mount

# let firewall accept all traffic from other nodes:
# iptables -I INPUT -p all -s <peer ip-address> -j ACCEPT
# configure trusted pool on each server:  gluster peer probe <peer>

# on each server make subdir:
mkdir -p /data/brick1/gv0
mkdir -p /data/brick1/gv1
mkdir -p /data/brick1/gv2
mkdir -p /data/brick1/gv3

# on any one of the servers create volume:
#gluster volume create gv0 replica 2 server1:/data/brick1/gv0 server2:/data/brick1/gv0
#gluster volume create gv1 replica 2 server1:/data/brick1/gv1 server2:/data/brick1/gv1
#gluster volume create gv2 replica 2 server1:/data/brick1/gv2 server2:/data/brick1/gv2
#gluster volume create gv3 replica 2 server1:/data/brick1/gv3 server2:/data/brick1/gv3
gluster volume create gv0 192.168.1.10:/data/brick1/gv0
gluster volume create gv1 192.168.1.10:/data/brick1/gv1
gluster volume create gv2 192.168.1.10:/data/brick1/gv2
gluster volume create gv3 192.168.1.10:/data/brick1/gv3
gluster volume start gv0
gluster volume start gv1
gluster volume start gv2
gluster volume start gv3
gluster volume info
# test it
# mount -t glusterfs server1:/gv0 /mnt

chown 2000 /data/brick1/gv0

add the glusterfs endpoints (server addresses)
kubectl create -f glusterfs-endpoints.json
verify:
kubectl get endpoints

# create a service for the glusterfs endpoints so that they will persist:
kubectl create -f glusterfs-service.json


# To use glusterfs storage in a pod (path is glusterfs volume name):
#"volumes": [
#  {
#    "name": "glusterfsvol",
#    "glusterfs": {
#      "endpoints": "glusterfs-cluster",
#      "path": "kube_vol",
#      "readWrite": true
#    }
#  }
#]

# to see if gluster volumes are used:
#kubectl exec glusterfs -- mount | grep gluster


# OR RUN gluster in pods! https://github.com/gluster/gluster-kubernetes

build custom Kibana-image to support Logtrail from Kibana dockerfile on every node as "localhost/kibana-logtrail:6.1.0"
docker build -t localhost/kibana-logtrail:6.1.0 kibana

monitoring:

kubectl delete -f es-ingress.yaml
kubectl delete -f es-service.yml
kubectl delete -f es-deployment.yml
kubectl delete -f es-serviceaccount.yml
kubectl delete -f es-clusterrolebinding.yml
kubectl delete -f es-clusterrole.yml

kubectl create -f es-clusterrole.yml
kubectl create -f es-clusterrolebinding.yml
kubectl create -f es-serviceaccount.yml
kubectl create -f es-deployment.yml
kubectl create -f es-service.yml
kubectl create -f es-ingress.yaml

kubectl create -f logstash-config.yaml
kubectl create -f logstashyaml-config.yaml
kubectl create -f logstash-deployment.yaml
kubectl create -f logstash-service.yaml

kubectl delete -f filebeat-ds.yaml
kubectl delete -f filebeat-config.yaml
kubectl delete -f filebeat-serviceaccount.yaml
kubectl delete -f filebeat-rolebinding.yaml
kubectl delete -f filebeat-role.yaml

kubectl create -f filebeat-role.yaml
kubectl create -f filebeat-rolebinding.yaml
kubectl create -f filebeat-serviceaccount.yaml
kubectl create -f filebeat-config.yaml
kubectl create -f filebeat-ds.yaml

kubectl delete -f influxdb-service.yaml
kubectl delete -f influxdb-deployment.yaml
kubectl delete -f influxdb-serviceaccount.yaml
kubectl delete -f influxdb-rolebinding.yaml
kubectl delete -f influxdb-role.yaml

kubectl create -f influxdb-role.yaml
kubectl create -f influxdb-rolebinding.yaml
kubectl create -f influxdb-serviceaccount.yaml
kubectl create -f influxdb-deployment.yaml
kubectl create -f influxdb-service.yaml

kubectl delete -f telegraf-ds.yaml
kubectl delete -f tgconfig.yaml
kubectl delete -f telegraf-serviceaccount.yaml
kubectl delete -f telegraf-rolebinding.yaml
kubectl delete -f telegraf-role.yaml

kubectl create -f telegraf-role.yaml
kubectl create -f telegraf-rolebinding.yaml
kubectl create -f telegraf-serviceaccount.yaml
kubectl create -f tgconfig.yaml
kubectl create -f telegraf-ds.yaml

kubectl delete -f kapacitor-deployment.yaml
kubectl delete -f kapacitor-config.yaml
kubectl delete -f kapacitor-serviceaccount.yaml
kubectl delete -f kapacitor-rolebinding.yaml
kubectl delete -f kapacitor-role.yaml

kubectl create -f kapacitor-role.yaml
kubectl create -f kapacitor-rolebinding.yaml
kubectl create -f kapacitor-serviceaccount.yaml
kubectl create -f kapacitor-config.yaml
kubectl create -f kapacitor-deployment.yaml

kubectl delete -f chronograf-ingress.yaml
kubectl delete -f chronograf-service.yaml
kubectl delete -f chronograf-deployment.yaml
kubectl delete -f chronograf-serviceaccount.yaml
kubectl delete -f chronograf-rolebinding.yaml
kubectl delete -f chronograf-role.yaml

kubectl create -f chronograf-role.yaml
kubectl create -f chronograf-rolebinding.yaml
kubectl create -f chronograf-serviceaccount.yaml
kubectl create -f chronograf-deployment.yaml
kubectl create -f chronograf-service.yaml
kubectl create -f chronograf-ingress.yaml

kubectl delete -f kibana-ingress.yaml
kubectl delete -f kibana-service.yaml
kubectl delete -f kibana-deployment.yaml
kubectl delete -f kibana-serviceaccount.yaml
kubectl delete -f kibana-clusterrolebinding.yaml
kubectl delete -f kibana-clusterrole.yaml

kubectl create -f kibana-clusterrole.yaml
kubectl create -f kibana-clusterrolebinding.yaml
kubectl create -f kibana-serviceaccount.yaml
kubectl create -f kibana-deployment.yaml
kubectl create -f kibana-service.yaml
kubectl create -f kibana-ingress.yaml
#kubectl apply -f https://raw.githubusercontent.com/elastic/beats/master/deploy/kubernetes/filebeat-kubernetes.yaml
# check status
kubectl --namespace=kube-system get ds/filebeat
