
Kubernetes cluster setup using kubeadm

https://kubernetes.io/docs/setup/independent/install-kubeadm/
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Install Ubuntu 16.04 without swap
Set private network-adapter (192.168.1.10 for master)
sudo apt-get install openssh-server git
sudo systemctl enable ssh
sudo systemctl disable ufw
sudo systemctl stop ufw

sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get -y update
sudo apt-get -y install docker-ce
sudo apt-get -y install git docker-compose curl

# Voor Centos:
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum -y install docker-ce epel-release
sudo systemctl enable docker && sudo systemctl start docker
sudo yum install -y python-pip curl
sudo pip install docker-compose
disable selinux
sudo systemctl disable firewalld

ALL MACHINES /etc/sysctl.conf:
vm.max_map_count=262144
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward = 1

CENTOS:
edit secure_path in /etc/sudoers to include /usr/local/bin

sudo su -

#### COREOS ######


add static interface:

sudo echo '[Match]
Name=enp0s9

[Network]
Address=192.168.1.10/24
Gateway=192.168.56.1

' > /etc/systemd/network/static.network

sudo systemctl restart systemd-networkd


#!/bin/bash
set -o nounset -o errexit

RELEASE="$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)"
CNI_VERSION="v0.6.0"

mkdir -p /opt/bin
cd /opt/bin
curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl}
chmod +x {kubeadm,kubelet,kubectl}

mkdir -p /opt/cni/bin
curl -L "https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-amd64-${CNI_VERSION}.tgz" | tar -C /opt/cni/bin -xz

BRANCH="release-$(cut -f1-2 -d .<<< "${RELEASE##v}")"
cd "/etc/systemd/system/"
curl -L "https://raw.githubusercontent.com/kubernetes/kubernetes/${BRANCH}/build/debs/kubelet.service" | sed 's:/usr/bin:/opt/bin:g' > kubelet.service
mkdir -p "/etc/systemd/system/kubelet.service.d"
cd "/etc/systemd/system/kubelet.service.d"
curl -L "https://raw.githubusercontent.com/kubernetes/kubernetes/${BRANCH}/build/debs/10-kubeadm.conf" | sed 's:/usr/bin:/opt/bin:g' > 10-kubeadm.conf

sudo hostnamectl set-hostname k8smaster
sudo bash -c "echo '192.168.1.10 k8smaster.localdomain k8smaster' >> /etc/hosts"

sudo kubeadm init --apiserver-advertise-address=192.168.1.10 --pod-network-cidr=10.244.0.0/16
export kubever=$(kubectl version | base64 | tr -d '\n')
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"
#sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
sudo systemctl enable docker kubelet

sudo su

#mkdir -p /etc/glusterfs
#mkdir -p /var/lib/glusterd
#mkdir -p /var/log/glusterfs
Alle nodes:
mkdir -p /data/rook

#sudo docker run -v /etc/glusterfs:/etc/glusterfs:z --name gluster -v /var/lib/glusterd:/var/lib/glusterd:z -v /var/log/glusterfs:/var/log/glusterfs:z -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /data:/data -d --privileged=true --net=host --restart always -v /dev/:/dev gluster/gluster-centos

install something such as rook for hyperconverged CEPH storage or run an NFS server (Ganesha)

add following to /etc/systemd/system/kubelet.service.d/10-kubeadm.conf and restart
--volume-plugin-dir=/var/lib/kubelet/volumeplugins
sudo mkdir -p /var/lib/kubelet/volumeplugins
sudo rm -rf /var/lib/rook/*
kubectl create -f rook-operator.yaml
kubectl create -f rook-cluster.yaml
kubectl create -f rook-storageclass.yaml
kubectl create -f rook-filesystem.yaml

sudo mkdir -p /data/rook/elasticsearch
chown 2000 /data/rook/elasticsearch

sudo docker run -v /etc/glusterfs:/etc/glusterfs:z --name ganesha -v /var/lib/glusterd:/var/lib/glusterd:z -v /var/log/glusterfs:/var/log/glusterfs:z -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /data:/nfsdata -d --privileged=true --net=host --restart always -v /dev/:/dev -e GANESHA_EXPORT=/nfsdata -e GANESHA_ACCESS=192.168.1.0/24 -e GANESHA_ROOT_ACCESS=192.168.1.0/24 mitcdh/nfs-ganesha

sudo docker exec -ti gluster sh
gluster volume create gv0 192.168.1.10:/data/brick1/gv0 force
gluster volume create gv1 192.168.1.10:/data/brick1/gv1 force
gluster volume create gv2 192.168.1.10:/data/brick1/gv2 force
gluster volume create gv3 192.168.1.10:/data/brick1/gv3 force
gluster volume create gv4 192.168.1.10:/data/brick1/gv4 force
gluster volume start gv0
gluster volume start gv1
gluster volume start gv2
gluster volume start gv3
gluster volume start gv4
gluster volume set gv0 nfs.disable off
gluster volume set gv1 nfs.disable off
gluster volume set gv2 nfs.disable off
gluster volume set gv3 nfs.disable off
gluster volume set gv4 nfs.disable off
gluster volume info



setup shared storage. Example Gluster (from https://launchpad.net/~gluster):

on gluster server:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-server
CENTOS:
yum -y install centos-release-gluster
yum -y install glusterfs-server
systemctl enable glusterd.service
systemctl start glusterd.service

on nodes:
sudo add-apt-repository ppa:gluster/glusterfs-3.12
sudo apt-get update
sudo apt-get install glusterfs-client
CENTOS:
yum -y install centos-release-gluster
yum -y install glusterfs
yum -y install glusterfs-fuse

# glusterfs uses /var/lib/glusterd for state
# Make gluster storage volume:

parted -s /dev/xvdz mklabel msdos
parted -s /dev/xvdz mkpart primary 1 100%

mkfs.xfs -i size=512 /dev/xvdz1
mkdir -p /data/brick1
echo '/dev/xvdz1 /data/brick1 xfs defaults 1 2' >> /etc/fstab
mount -a && mount

# let firewall accept all traffic from other nodes:
# iptables -I INPUT -p all -s <peer ip-address> -j ACCEPT
# configure trusted pool on each server:  gluster peer probe <peer>

# on each server make subdir:
mkdir -p /data/brick1/gv0
mkdir -p /data/brick1/gv1
mkdir -p /data/brick1/gv2
mkdir -p /data/brick1/gv3
mkdir -p /data/brick1/gv4

# on any one of the servers create volume:
#gluster volume create gv0 replica 2 server1:/data/brick1/gv0 server2:/data/brick1/gv0
#gluster volume create gv1 replica 2 server1:/data/brick1/gv1 server2:/data/brick1/gv1
#gluster volume create gv2 replica 2 server1:/data/brick1/gv2 server2:/data/brick1/gv2
#gluster volume create gv3 replica 2 server1:/data/brick1/gv3 server2:/data/brick1/gv3
#gluster volume create gv4 replica 2 server1:/data/brick1/gv4 server2:/data/brick1/gv4
gluster volume create gv0 192.168.1.10:/data/brick1/gv0 force
gluster volume create gv1 192.168.1.10:/data/brick1/gv1 force
gluster volume create gv2 192.168.1.10:/data/brick1/gv2 force
gluster volume create gv3 192.168.1.10:/data/brick1/gv3 force
gluster volume create gv4 192.168.1.10:/data/brick1/gv4 force
gluster volume start gv0
gluster volume start gv1
gluster volume start gv2
gluster volume start gv3
gluster volume start gv4
gluster volume info
# test it
# mount -t glusterfs server1:/gv0 /mnt

chown 2000 /data/brick1/gv0


# OR RUN gluster in pods! https://github.com/gluster/gluster-kubernetes


############## KUBEADM ########################
apt-get update && apt-get install -y apt-transport-https
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl

# Voor Centos:
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
setenforce 0
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
reboot

ON MASTER ONLY:
kubeadm init --apiserver-advertise-address=192.168.1.10 --pod-network-cidr=10.244.0.0/16


install a pod network addon:
export kubever=$(kubectl version | base64 | tr -d '\n')
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"

#kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
#kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/rbac.yaml
#kubectl apply -f https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.7/canal.yaml


following command should show kube-dns pod running:
kubectl get pods --all-namespaces

wait until stabilized
#You can now join any number of machines by running the following on each node
#as root:
#
#  kubeadm join --token 003f03.441f4f6c5d87d22c 192.168.1.10:6443 --discovery-token-ca-cert-hash sha256:5b7159073875aa9f9f947173c169dfc10c93b3813c56ee2d60c308747e23fd2f


If you are using VirtualBox (directly or via Vagrant), you will need to ensure that hostname -i returns a routable IP address. also the first ethernet adapter should be a cluster-reachable one (internal network)
#################### KUBEADM END #################################


############### RANCHER 2.0 #####################
on master:
mkdir -p /opt/rancher/mysql
sudo docker run -d --restart=unless-stopped -v /opt/rancher/mysql:/var/lib/mysql -p 8080:8080 rancher/server:preview
do the rest from the web interface



add the glusterfs endpoints (server addresses)
kubectl create -f glusterfs-endpoints.json
verify:
kubectl get endpoints

# create a service for the glusterfs endpoints so that they will persist:
kubectl create -f glusterfs-service.json


# To use glusterfs storage in a pod (path is glusterfs volume name):
#"volumes": [
#  {
#    "name": "glusterfsvol",
#    "glusterfs": {
#      "endpoints": "glusterfs-cluster",
#      "path": "kube_vol",
#      "readWrite": true
#    }
#  }
#]

# to see if gluster volumes are used:
#kubectl exec glusterfs -- mount | grep gluster


For allowing pods to be scheduled on master itself:
kubectl taint nodes --all node-role.kubernetes.io/master-

join nodes with the command output by kubeadm

deploy kubernetes dashboard:
kubectl create -f dashboard-rbac.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
OR without SSL:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/alternative/kubernetes-dashboard.yaml

installeer eventueel heapster et al:
#kubectl create -f https://git.io/weave-kube
#$ kubectl create -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/monitoring-standalone/v1.2.0.yaml

#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml
#kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml

install traefik
kubectl apply -f traefik-rbac.yaml
kubectl apply -f traefik-daemonset.yaml
kubectl apply -f traefik-web-ui.yaml

# weave scope
kubectl apply --namespace kube-system -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"
kubectl create -f weavescope-ingress.yaml
run heapster rbac:
#kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml

maar beter TICK stack en ELK

proxy the dashboard:
kubectl proxy

make a tunnel and access it as localhost:
http://localhost:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy/

build custom Kibana-image to support Logtrail from Kibana dockerfile on every node as "localhost/kibana-logtrail:6.1.0"
docker build -t localhost/kibana-logtrail:6.1.0 kibana

monitoring:

kubectl delete -f es-ingress.yaml
kubectl delete -f es-service.yml
kubectl delete -f es-deployment.yml
kubectl delete -f es-serviceaccount.yml
kubectl delete -f es-clusterrolebinding.yml
kubectl delete -f es-clusterrole.yml
kubectl delete -f es-filesystem.yaml

kubectl create -f es-filesystem.yaml
-- wait until provisioned (kubectl get pods -n rook | grep elast)
kubectl create -f es-clusterrole.yml
kubectl create -f es-clusterrolebinding.yml
kubectl create -f es-serviceaccount.yml
kubectl create -f es-deployment.yml
kubectl create -f es-service.yml
kubectl create -f es-ingress.yaml

kubectl create -f logstash-config.yaml
kubectl create -f logstashyaml-config.yaml
kubectl create -f logstash-deployment.yaml
kubectl create -f logstash-service.yaml

kubectl delete -f filebeat-ds.yaml
kubectl delete -f filebeat-config.yaml
kubectl delete -f filebeat-serviceaccount.yaml
kubectl delete -f filebeat-rolebinding.yaml
kubectl delete -f filebeat-role.yaml

kubectl create -f filebeat-role.yaml
kubectl create -f filebeat-rolebinding.yaml
kubectl create -f filebeat-serviceaccount.yaml
kubectl create -f filebeat-config.yaml
kubectl create -f filebeat-ds.yaml

kubectl delete -f influxdb-service.yaml
kubectl delete -f influxdb-deployment.yaml
kubectl delete -f influxdb-serviceaccount.yaml
kubectl delete -f influxdb-rolebinding.yaml
kubectl delete -f influxdb-role.yaml

kubectl create -f influxdb-role.yaml
kubectl create -f influxdb-rolebinding.yaml
kubectl create -f influxdb-serviceaccount.yaml
kubectl create -f influxdb-deployment.yaml
kubectl create -f influxdb-service.yaml

kubectl delete -f telegraf-ds.yaml
kubectl delete -f tgconfig.yaml
kubectl delete -f telegraf-serviceaccount.yaml
kubectl delete -f telegraf-rolebinding.yaml
kubectl delete -f telegraf-role.yaml

kubectl create -f telegraf-role.yaml
kubectl create -f telegraf-rolebinding.yaml
kubectl create -f telegraf-serviceaccount.yaml
kubectl create -f tgconfig.yaml
kubectl create -f telegraf-ds.yaml

kubectl delete -f kapacitor-deployment.yaml
kubectl delete -f kapacitor-config.yaml
kubectl delete -f kapacitor-serviceaccount.yaml
kubectl delete -f kapacitor-rolebinding.yaml
kubectl delete -f kapacitor-role.yaml

kubectl create -f kapacitor-role.yaml
kubectl create -f kapacitor-rolebinding.yaml
kubectl create -f kapacitor-serviceaccount.yaml
kubectl create -f kapacitor-config.yaml
kubectl create -f kapacitor-deployment.yaml

kubectl delete -f chronograf-ingress.yaml
kubectl delete -f chronograf-service.yaml
kubectl delete -f chronograf-deployment.yaml
kubectl delete -f chronograf-serviceaccount.yaml
kubectl delete -f chronograf-rolebinding.yaml
kubectl delete -f chronograf-role.yaml

kubectl create -f chronograf-role.yaml
kubectl create -f chronograf-rolebinding.yaml
kubectl create -f chronograf-serviceaccount.yaml
kubectl create -f chronograf-deployment.yaml
kubectl create -f chronograf-service.yaml
kubectl create -f chronograf-ingress.yaml

kubectl delete -f kibana-ingress.yaml
kubectl delete -f kibana-service.yaml
kubectl delete -f kibana-deployment.yaml
kubectl delete -f kibana-serviceaccount.yaml
kubectl delete -f kibana-clusterrolebinding.yaml
kubectl delete -f kibana-clusterrole.yaml

kubectl create -f kibana-clusterrole.yaml
kubectl create -f kibana-clusterrolebinding.yaml
kubectl create -f kibana-serviceaccount.yaml
kubectl create -f kibana-deployment.yaml
kubectl create -f kibana-service.yaml
kubectl create -f kibana-ingress.yaml
#kubectl apply -f https://raw.githubusercontent.com/elastic/beats/master/deploy/kubernetes/filebeat-kubernetes.yaml
# check status
kubectl --namespace=kube-system get ds/filebeat
